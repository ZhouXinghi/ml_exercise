{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "# ==================================== 0. Dataloader\n",
    "\n",
    "class CIFAR10Set(torchvision.datasets.CIFAR10):\n",
    "    \"\"\"\n",
    "    Get a subset of CIFAR10 dataset, according to the passed indices\n",
    "    \"\"\"\n",
    "    def __init__(self, root, train, transform, download, idx=None):\n",
    "        super().__init__(root=root, train=train, transform=transform, download=download)\n",
    "        if idx is None:\n",
    "            return\n",
    "        self.data = self.data[idx]\n",
    "        targets_array = np.array(self.targets)\n",
    "        self.targets = targets_array[idx].tolist()\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "n_train = 45_000\n",
    "train_set = CIFAR10Set(idx=range(n_train), root=\"./data\", train=True, transform=transform_train, download=True)\n",
    "val_set = CIFAR10Set(idx =range(n_train, 50000), root=\"./data\", train=True, transform=transform_eval, download=True)\n",
    "test_set = CIFAR10Set(root=\"./data\", train=True, transform=transform_eval, download=True)\n",
    "\n",
    "#  print(train_set[20001][0].shape)\n",
    "#  print(train_set[20001][1])\n",
    "#  print(test_set[333][0].shape)\n",
    "#  print(test_set[333][1])\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, pin_memory=True)\n",
    "dataloaders[\"val\"] = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, pin_memory=True)\n",
    "dataloaders[\"test\"] = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "#  print(device)\n",
    "\n",
    "# for x, y in dataloaders[\"train\"]:\n",
    "#     print(x.shape, y)\n",
    "#     break;\n",
    "\n",
    "\n",
    "\n",
    "# ==================================== 1. Dropout\n",
    "\n",
    "def my_dropout(X, p):\n",
    "    assert 0 <= p <= 1\n",
    "    X = X.float()\n",
    "    if p == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) < 1 - p).float()\n",
    "    return X * mask / (1 - p)\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        p: float, dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: PyTorch tensor, arbitrary shape\n",
    "        Returns:\n",
    "            PyTorch tensor, same shape as input\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            mask = input.new_empty(input.shape)\n",
    "            mask.bernoulli_(1 - self.p)\n",
    "            return mask * input / (1 - self.p)\n",
    "        return input\n",
    "\n",
    "\n",
    "#  X = torch.arange(9).reshape(3, 3)\n",
    "#  X.bernoulli(0.5)\n",
    "#  print(X)\n",
    "#  X.bernoulli_(0.5)\n",
    "#  print(X)\n",
    "#  torch.bernoulli(X, 0.5)\n",
    "#  for i in range(10):\n",
    "#      X_drop = Dropout(0.5)(X)\n",
    "#      print(X_drop)\n",
    "#\n",
    "#  # Test dropout\n",
    "#  test = torch.rand(10_000)\n",
    "#  dropout = Dropout(0.2)\n",
    "#  test_dropped = dropout(test)\n",
    "#\n",
    "#  # These assertions can in principle fail due to bad luck, but\n",
    "#  # if implemented correctly they should almost always succeed.\n",
    "#  assert np.isclose(test_dropped.mean().item(), test.mean().item(), atol=1e-2)\n",
    "#  assert np.isclose((test_dropped > 0).float().mean().item(), 0.8, atol=1e-2)\n",
    "\n",
    "# ============================================ 2. Batch normalization\n",
    "class BatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Args: num_features, Number of features to calculate batch statistics for.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.parameter.Parameter(torch.ones(num_features))\n",
    "        self.beta = nn.parameter.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: Pytorch tensor, shape (N, C, L)\n",
    "        Return:\n",
    "            Pytorch tensor, shame shape as input\n",
    "        \"\"\"\n",
    "        eps = 1e-5\n",
    "        mean = torch.mean(input, dim=[0, 2], keepdim=True)\n",
    "        std = torch.std(input, dim=[0, 2], keepdim=True)\n",
    "\n",
    "        input_normalized = (input - mean) / (std + eps)\n",
    "        return input_normalized * self.gamma[None, :, None] + self.beta[None, :, None]\n",
    "\n",
    "#  torch.random.manual_seed(42)\n",
    "#  test = torch.randn(8, 2, 4)\n",
    "#\n",
    "#  b1 = BatchNorm(2)\n",
    "#  test_b1 = b1(test)\n",
    "#  b2 = nn.BatchNorm1d(2)\n",
    "#  test_b2 = b2(test)\n",
    "#\n",
    "#  assert torch.allclose(test_b1, test_b2, rtol=0.2)\n",
    "\n",
    "# ============================================ 3. ResNet\n",
    "class Oper(nn.Module):\n",
    "    def __init__(self, func=None):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.func is not None:\n",
    "            return self.func(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first convolution\n",
    "        stride: Stride size of the first convolution, used for downsampling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        if in_channels != out_channels or stride > 1:\n",
    "            self.skip = Oper(\n",
    "                lambda x: F.pad(\n",
    "                    x[:, :, ::stride, ::stride],\n",
    "                    (0, 0, 0, 0, 0, out_channels - in_channels),\n",
    "                    mode=\"constant\",\n",
    "                    value=0\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.skip = Oper(func=None)\n",
    "\n",
    "        self.path = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                    )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.relu(self.path(input) + self.skip(input))\n",
    "\n",
    "#  for x, y in dataloaders[\"train\"]:\n",
    "#      x, y = x.to(device), y.to(device)\n",
    "#      block = ResidualBlock(3, 16, 2)\n",
    "#      block.to(device)\n",
    "#      x_new = block(x)\n",
    "#      print(x.shape)\n",
    "#      print(x_new.shape)\n",
    "#      break\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of resudual blocks.\n",
    "\n",
    "    Args:\n",
    "        in_channels: The number of channels (feature maps) of the incoming embedding\n",
    "        out_channels: The number of channels after the first layer\n",
    "        stride: Stride size of the first layer, used for downsampling\n",
    "        num_blocks: Number of residual blocks\n",
    "    \"\"\"\n",
    "    #  def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "    #      super().__init__()\n",
    "    #      self.block1 = ResidualBlock(in_channels, out_channels, stride)\n",
    "    #      self.block2 = ResidualBlock(in_channels=out_channels, out_channels=out_channels, stride=1)\n",
    "    #\n",
    "    #  def forward(self, input):\n",
    "    #      x = self.block2(self.block1(input))\n",
    "    #      return x\n",
    "\n",
    "    # iportant: We cannot use python list here. We must use nn.ModuleList, otherwise\n",
    "    # the modules cannot be transfered to GPU.\n",
    "    def __init__(self, in_channels, out_channels, stride, num_blocks):\n",
    "        super().__init__()\n",
    "        block_list = [ResidualBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(num_blocks - 1):\n",
    "            block_list.append(ResidualBlock(in_channels=out_channels, out_channels=out_channels, stride=1))\n",
    "        self.block_list = nn.ModuleList(block_list)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for block in self.block_list:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "#  for x, y in dataloaders[\"train\"]:\n",
    "#      x, y = x.to(device), y.to(device)\n",
    "#      stack = ResidualStack(in_channels=3, out_channels=16, stride=2, num_blocks=5)\n",
    "#      stack.to(device)\n",
    "#      print(x.shape)\n",
    "#      x_new = stack(x)\n",
    "#      print(x.shape)\n",
    "#      print(x_new.shape)\n",
    "#      break\n",
    "\n",
    "n = 5 \n",
    "num_classes = 10\n",
    "resnet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(),\n",
    "    ResidualStack(16, 16, stride=1, num_blocks=n),\n",
    "    ResidualStack(16, 32, stride=2, num_blocks=n),\n",
    "    ResidualStack(32, 64, stride=2, num_blocks=n),\n",
    "    nn.AdaptiveAvgPool2d(output_size=1), \n",
    "    Oper(lambda x: x.squeeze()), \n",
    "    nn.Linear(64, num_classes)\n",
    ")\n",
    "\n",
    "#  # target output size of 5x7\n",
    "#  m = nn.AdaptiveMaxPool2d((5,7))\n",
    "#  input = torch.randn(1, 64, 8, 9)\n",
    "#  output = m(input)\n",
    "#  print(output.shape)\n",
    "#  # target output size of 7x7 (square)\n",
    "#  m = nn.AdaptiveMaxPool2d(7)\n",
    "#  input = torch.randn(1, 64, 10, 9)\n",
    "#  output = m(input)\n",
    "#  print(output.shape)\n",
    "#  # target output size of 10x7\n",
    "#  m = nn.AdaptiveMaxPool2d((None, 7))\n",
    "#  input = torch.randn(1, 64, 10, 9)\n",
    "#  output = m(input)\n",
    "#  print(output.shape)\n",
    "\n",
    "#  for x, y in dataloaders[\"train\"]:\n",
    "#      print(x.shape)\n",
    "#      x_new = resnet(x)\n",
    "def initialize_weight(module): \n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity=\"relu\")\n",
    "    elif isinstance(module, nn.BatchNorm2d): \n",
    "        nn.init.constant_(module.weight, 1) \n",
    "        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "resnet.apply(initialize_weight)\n",
    "\n",
    "print(device)\n",
    "resnet.to(device)\n",
    "#  print(resnet)\n",
    "\n",
    "#  for x, y in dataloaders[\"train\"]:\n",
    "#      x, y = x.to(device), y.to(device)\n",
    "#      print(x.shape)\n",
    "#      x_new = resnet(x)\n",
    "#      print(x.shape)\n",
    "#      print(x_new.shape)\n",
    "#      break\n",
    "\n",
    "# ================================================ 4. Training\n",
    "\n",
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    \"\"\" \n",
    "    Run one epoch of training or evaluation. \n",
    "\n",
    "    Args: \n",
    "        model: The model used for prediction \n",
    "        optimizer: optimization algorithm for the model \n",
    "        dataloader: Dataloader providing the data to run our model on \n",
    "        train: Whether this epoch is used for training or evaluation \n",
    "\n",
    "    Returns: \n",
    "        Loss and accuracy for this epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        model.train() \n",
    "    else: \n",
    "        model.eval() \n",
    "\n",
    "    loss_sum = 0.0 \n",
    "    num_correct_sum = 0.0\n",
    "    for xb, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device) \n",
    "        if train:\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        num_correct = (preds == yb).sum()\n",
    "\n",
    "        if train: \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() \n",
    "        num_correct_sum += num_correct\n",
    "    #  print(loss_sum / len(dataloader.dataset), num_correct_sum / len(dataloader.dataset))\n",
    "    return loss_sum / len(dataloader.dataset), num_correct_sum / len(dataloader.dataset)\n",
    "\n",
    "resnet.to(device)\n",
    "#  run_epoch(resnet, None, dataloaders[\"val\"], train=False)\n",
    "\n",
    "\n",
    "def fit(model, optimizer, lr_scheduler, dataloaders, max_epochs, patience): \n",
    "    \"\"\" \n",
    "    Fit the given model on the dataset \n",
    "\n",
    "    Args: \n",
    "        model: The model used for prediction \n",
    "        optimizer: optimization algorithm for the model \n",
    "        lr_scheduler: Learning rate scheduler that improves training in late epochs with learning rate decay \n",
    "        dataloaders: Dataloaders for training and validation \n",
    "        max_epochs: Maximum number of epochs for training \n",
    "        patience: Number of epochs to wait with early stopping the training if validation accuracy has decreased \n",
    "    \"\"\"\n",
    "    \n",
    "    best_acc = 0\n",
    "    curr_patience = 0\n",
    "    best_weights = None\n",
    "    for epoch in range(max_epochs): \n",
    "        train_loss, train_acc = run_epoch(model, optimizer, dataloaders[\"train\"], train=True)\n",
    "        print(f\"Epoch {epoch + 1} / {max_epochs}, train loss: {train_loss:.2e}, accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "        val_loss, val_acc = run_epoch(model, optimizer, dataloaders[\"val\"], train=False)\n",
    "        print(f\"Epoch {epoch + 1} / {max_epochs}, val loss: {val_loss:.2e}, accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc \n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            curr_patience = 0\n",
    "        else:\n",
    "            curr_patience += 1\n",
    "\n",
    "        if curr_patience >= patience:\n",
    "            break \n",
    "        \n",
    "        model.load_state_dict(best_weights)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 50], gamma=0.1)\n",
    "fit(resnet, optimizer, lr_scheduler, dataloaders, max_epochs=200, patience=5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
