{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise 8: PyTorch from the bottom up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Jeremy Howard's PyTorch tutorial \"What is torch.nn really?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will start at [PyTorch](https://pytorch.org/docs)'s lowest layer and then gradually introduce functions and features until we arrive at `nn.Sequential`. Lower layers give you more control over what you want to do, while higher layers allow for faster implementations. So in practice you have to choose at which layer you want to work. Moreover, knowing how the lower layers work will give you a better understanding of what is happening behind the scenes when working with the higher level abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "In this tutorial we will be working with the MNIST dataset. This is a classic dataset consisting of black and white images of hand-drawn digits.\n",
    "\n",
    "We will use [torchvision](https://pytorch.org/docs/stable/torchvision) to download the dataset. Torchvision also provides a lot of functionality for data preprocessing and augmentation, which is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dev = torchvision.datasets.MNIST(\"./data\", train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data $\\mathbf{X}$ and targets $\\mathbf{y}$ are saved in `data` and `targets`, so we will at first just extract these and work with the raw data. We also convert them to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = mnist_dev.data / 255.0\n",
    "y_dev = mnist_dev.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST consists of 60,000 28x28 images, each corresponding to a single digit (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feab0757f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_dev.shape)\n",
    "print(y_dev.shape)\n",
    "\n",
    "plt.imshow(x_dev[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that setting `train=True` gives you the development set, i.e. both training and validation data. So we need to split this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 50000\n",
    "x_train, y_train = x_dev[:ntrain].flatten(1), y_dev[:ntrain]\n",
    "x_val, y_val = x_dev[ntrain:].flatten(1), y_dev[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.tensor\n",
    "PyTorch uses its own `torch.tensor` datatype. This is very similar to a Numpy Array, but can also be moved to and used for calculations on a GPU, and supports storing gradient information and hence dynamic backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by manually setting up an affine layer. The special function `requires_grad` tells PyTorch that these weights require gradients. PyTorch will then record all operations done on the tensor, so backpropagation can be done automatically. Thanks to this ability we can use any normal function as a model in PyTorch.\n",
    "\n",
    "Note that we initialize the weights via Xavier (Glorot) initialization. We only activate gradients after initialization, since we don't want gradients for that.\n",
    "\n",
    "Appending a function with `_` in PyTorch denotes in-place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 28 * 28\n",
    "num_classes = 10\n",
    "\n",
    "weight = torch.empty(num_features, num_classes).uniform_(-1, 1) * np.sqrt(2 / (num_features + num_classes))\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(num_classes, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0014,  0.0033,  0.0457,  ..., -0.0453,  0.0142,  0.0004],\n",
       "        [ 0.0412, -0.0019,  0.0481,  ...,  0.0281, -0.0409, -0.0437],\n",
       "        [ 0.0302, -0.0365,  0.0487,  ..., -0.0317, -0.0244,  0.0490],\n",
       "        ...,\n",
       "        [-0.0333,  0.0225, -0.0300,  ..., -0.0418, -0.0376, -0.0087],\n",
       "        [ 0.0299,  0.0281,  0.0135,  ...,  0.0350,  0.0279,  0.0438],\n",
       "        [-0.0491,  0.0379,  0.0231,  ..., -0.0297,  0.0175,  0.0456]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use these weights to create a simple linear model (i.e. logistic regression). We furthermore define a loss (negative log-likelihood) for training and a function to obtain the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input: Tensor of features, shape (num_samples, num_features)\n",
    "    \n",
    "    Returns:\n",
    "        logits: Tensor of logits, shape (num_samples, num_classes)\n",
    "            logits[n, k] = log p(y_n = k | x_n)\n",
    "    \"\"\"\n",
    "    return log_softmax(input @ weight + bias)\n",
    "\n",
    "def log_softmax(input):\n",
    "    return input - input.logsumexp(-1, keepdims=True)\n",
    "\n",
    "def nll_loss(logits, target):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: Tensor of logits, shape (num_samples, num_classes)\n",
    "            logits[n, k] = log p(y_n = k | x_n)\n",
    "        target: Correct class labels, shape (num_samples,)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Tensor, average loss for the batch of data, shape ()\n",
    "    \"\"\"\n",
    "    return -logits[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_fn = nll_loss\n",
    "\n",
    "def get_accuracy(logits, target):\n",
    "    pred = logits.argmax(dim=-1)\n",
    "    return (pred == target).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model performs before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1719)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a training loop. In this loop we need to\n",
    "1. Get a mini-batch of data. When using dynamic computation graphs like in PyTorch it is important to choose a batch size that is large enough to leverage your hardware properly.\n",
    "2. Generate predictions with our model\n",
    "3. Calculate the loss\n",
    "4. Update the gradients via `loss.backward()`\n",
    "5. Update the `weight` and `bias` based on the gradients (optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(int(np.ceil(ntrain / batch_size))):\n",
    "        # Get minibatch\n",
    "        start_i = i * batch_size\n",
    "        end_i = min(start_i + batch_size, ntrain)\n",
    "        xb = x_train[start_i:end_i]  # [batch_size, num_features]\n",
    "        yb = y_train[start_i:end_i]  # [batch_size]\n",
    "        \n",
    "        # Generate predictions\n",
    "        logits = model(xb)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, yb)\n",
    "        \n",
    "        # Compute the gradients of the loss w.r.t. parameters (weight, bias)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Do gradient descent\n",
    "        with torch.no_grad():\n",
    "            weight -= learning_rate * weight.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we need! And now we can check if our performance has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2299, grad_fn=<NegBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it works! Wasn't this already way easier than with pure Numpy? But this is just the start. Now that we've implemented our model in the lowest level of PyTorch we can start to go up the ladder and make this even better and simpler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by replacing some of our hand-written functions with their professionally implemented counterparts in `torch.nn.functional`. This library contains all of the PyTorch functions (other parts contain the classes). It is commonly imported via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using `log_softmax` and `neg_loglikelihood` we can instead just use `F.cross_entropy`, which combines both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return input @ weight + bias\n",
    "\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss should still be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2299, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module\n",
    "Next we will use `nn.Module` and `nn.Parameter` for a clearer and more concise model definition and training loop. By subclassing `nn.Module` we obtain various convenience functions such as `.parameters()` and `.zero_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter((torch.empty(num_features, num_classes).uniform_(-1, 1) \n",
    "                                    * np.sqrt(2 / (num_features + num_classes))))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `LogRegression` is now a class we will have to first instantiate it before using it. We can then call it as if it were a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2898, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features, num_classes)\n",
    "print(loss_fn(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take advantage of `.parameters()` and `.zero_grad()` to make our training loop more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "num_epochs = 2\n",
    "\n",
    "def fit(model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(ntrain / batch_size))):\n",
    "            # Get minibatch\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, ntrain)\n",
    "            xb = x_train[start_i:end_i]  # [batch_size, num_features]\n",
    "            yb = y_train[start_i:end_i]  # [batch_size]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(xb)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, yb)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t. parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check if our results are similar to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually defining and initializing the affine layer, we can instead use the PyTorch class `nn.Linear`. PyTorch provides a wide range of predefined layers to simplify our code (and make it faster). On GitHub you will find layers for pretty much anything you might want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(num_features, num_classes)    \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.lin(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control weight initialization we must define a function and apply it to the model with `.apply(_)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if we still get the same results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2274, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(num_features, num_classes)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "fit(model, num_epochs=2)\n",
    "\n",
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.optim` provides various optimization algorithms. Here we will continue to use simple `SGD`, but you could just as easily switch to Adam or AMSgrad. Optimizers provide `.step()` and `.zero_grad()` methods, which allows us to make the last block in our `fit` function more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(ntrain / batch_size))):\n",
    "            # Get minibatch\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, ntrain)\n",
    "            xb = x_train[start_i:end_i]  # [batch_size, num_features]\n",
    "            yb = y_train[start_i:end_i]  # [batch_size]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(xb)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, yb)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t. parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "PyTorch also provides an abstract Dataset class for easier handling of various data. You can subclass this just like we subclassed `nn.Module`. A Dataset only needs to provide a `__len__` (which is called by Python's `len` function) and a `__getitem__` function for indexing the dataset.\n",
    "\n",
    "`TensorDataset` provides an easy way of converting tensors to datasets. This will make our data loading more concise, since we can handle both `x_train` and `y_train` simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, train_set, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil(ntrain / batch_size))):\n",
    "            # Get minibatch\n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, ntrain)\n",
    "            xb, yb = train_set[start_i:end_i]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(xb)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, yb)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t. parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(28 * 28, 10)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, train_set, num_epochs=2)\n",
    "\n",
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `DataLoader` automatically generates mini-batches for your training loop. It can run multiple workers in parallel and provides useful functionality such as data shuffling. You can create a `DataLoader` for any `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DataLoader makes our training loop a lot cleaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            # Generate predictions\n",
    "            logits = model(xb)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, yb)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t. parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1488, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9531)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(28 * 28, 10)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, train_loader, num_epochs=2)\n",
    "\n",
    "print(loss_fn(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Now that we have a training loop we can go ahead and do some real work. To avoid overfitting, enable early stopping and have some information for model development we always need a validation set.\n",
    "\n",
    "Since the validation set does not need backpropagation we can use 2x larger batches for it. Furthermore, we should shuffle our training data to avoid correlation between batches. This is not necessary (and would waste computation time) for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "dataloaders[\"val\"] = DataLoader(val_set, batch_size=2 * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to call `model.train()` before training and `model.eval()` before evaluation (inference), since some layers like dropout and batch normalization work differently in each mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for xb, yb in dataloaders[\"train\"]:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dataloaders[\"val\"]:\n",
    "                logits = model(xb)\n",
    "                # loss is a scala, why need to sum\n",
    "                loss += loss_fn(logits, yb).sum()\n",
    "            avg_loss = loss / len(dataloaders[\"val\"])\n",
    "        \n",
    "        print(f\"Epoch {epoch}: val_loss = {avg_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss = 0.293\n",
      "Epoch 1: val_loss = 0.282\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(28 * 28, 10)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "Using simple logistic regression (or an MLP) for images basically ignores the data's underlying structure. We can do much better than this by switching to a CNN. Since our training loop does not assume anything about the model we can train a CNN without any changes.\n",
    "\n",
    "Our CNN will consist of 3 convolutional layers, each using PyTorch's predefined `Conv2d` layer. At the End, we perform average pooling. Since `Conv2d` assumes a shape of `[batch_size, num_channels, height, width]` we need to reshape our input inside the model via `.view(_)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 784])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(num_channels, num_classes, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))  # [batch_size, num_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now furthermore use momentum in our optimizer to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss = 0.371\n",
      "Epoch 1: val_loss = 0.331\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_channels=16, num_classes=10)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "fit(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a class `nn.Sequential` for simplifying the definition of modules that only consist of a stack of layers. Since these are exactly the models we have been using so far we will now switch to this interface.\n",
    "\n",
    "Because not all functions are defined as PyTorch layers we will start by defining a module that just converts a function to a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our CNN in a more concise manner. Note that we now use `nn.AdaptiveAvgPool2d`, which allows us to specify the size of the output tensor instead of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(lambda x: x.view(-1, 1, 28, 28)),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss = 0.637\n",
      "Epoch 1: val_loss = 0.487\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "fit(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run significantly faster on a GPU than on a CPU, so you should always try to leverage that hardware. To do so, you need to move both your model and your data to the device.\n",
    "\n",
    "So let's first check if you have a GPU and choose the appropriate device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move our model to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Lambda()\n",
       "  (1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (2): ReLU()\n",
       "  (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): Conv2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (6): ReLU()\n",
       "  (7): AdaptiveAvgPool2d(output_size=1)\n",
       "  (8): Lambda()\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we redefine the DataLoader to pin the memory. This is a trick that will accelerate moving data between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "dataloaders[\"val\"] = DataLoader(val_set, batch_size=2 * batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to slightly change our training loop to send each batch to the device first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for xb, yb in dataloaders[\"train\"]:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dataloaders[\"val\"]:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                logits = model(xb)\n",
    "                loss += loss_fn(logits, yb).sum()\n",
    "            avg_loss = loss / len(dataloaders[\"val\"])\n",
    "        \n",
    "        print(f\"Epoch {epoch}: val_loss = {avg_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can run our CNN on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: val_loss = 0.357\n",
      "Epoch 1: val_loss = 0.231\n"
     ]
    }
   ],
   "source": [
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "fit(model, optimizer, dataloaders, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Great, so now we have a concise, but general training loop and know how to quickly define new models! Now let us sum up what we have learned during this journey:\n",
    "- `torch.tensor`: PyTorch tensors work like Numpy arrays, but can remember gradients and be sent to the GPU.\n",
    "- `torch.nn`\n",
    "    - `torch.nn.functional`: Provides various useful functions (non stateful) for training neural networks, e.g. activation and loss functions.\n",
    "    - `nn.Module`: Subclass from this to create a callable that acts like a function, but can remember state. It knows what `Parameter`s and submodules it contains and provides various functionality based on that.\n",
    "    - `nn.Parameter`: Wraps a tensor and tells the containing Module that it needs updating during backpropagation.\n",
    "    - `torch.nn`: Many useful layers are already implemented in this library, e.g. `nn.Linear` or `nn.Conv2d`.\n",
    "    - `nn.Sequential`: Provides an easy way of defining purely stacked modules.\n",
    "- `torch.optim`: Optimizers such as `SGD` or `Adam`, which let you easily update and train the `Parameter`s inside the passed model.\n",
    "- `Dataset`: Interface for data using only the `__len__` and `__getitem__` functions. Tensors can be converted into a `Dataset` by using `TensorDataset`.\n",
    "- `DataLoader`: Takes any `Dataset` and provides an iterator for returning mini-batches with various advanced functionality.\n",
    "- `GPU`: To use your GPU you need to move your model and each mini-batch to your GPU using `.to(device)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
