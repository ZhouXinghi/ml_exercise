{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dev = torchvision.datasets.MNIST(\"./data\", train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST(\"./data\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = mnist_dev.data / 255\n",
    "y_dev = mnist_dev.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1867e2f4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_dev.data.shape \n",
    "x_dev.shape\n",
    "y_dev.shape\n",
    "plt.imshow(x_dev[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 50000\n",
    "n_val = 10000 \n",
    "x_train, y_train = x_dev[:n_train].flatten(start_dim=1), y_dev[:n_train]\n",
    "x_val, y_val = x_dev[n_train:].flatten(start_dim=1), y_dev[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784]) torch.Size([10000, 784])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 28 * 28 \n",
    "num_classes = 10\n",
    "\n",
    "weight = torch.empty(num_features, num_classes, requires_grad=False).uniform_(-1, 1) * np.sqrt(6 / (num_features + num_classes))\n",
    "weight.requires_grad_()\n",
    "bias = torch.zeros(num_classes, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input: Tensor of features, shape (num_samples, num_features)\n",
    "    Returns:\n",
    "        logits: Tensor of logits, shape (num_samples, num_classes)\n",
    "            logits[n, k] = log p(y_n = k | x_n)\n",
    "    \"\"\"\n",
    "    # return torch.log_softmax(input @ weight + bias)\n",
    "    return (input @ weight + bias).log_softmax(dim=1)\n",
    "\n",
    "def log_softmax(input):\n",
    "    return input - input.logsumexp(dim=1, keepdims=True)\n",
    "\n",
    "def nll_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: Tensor of logits, shape (num_samples, num_classes)\n",
    "            logits[n, k] = log p(y_n = k | x_n)\n",
    "        targets: Correct class labels, shape (num_samples, )\n",
    "    Returns:\n",
    "        loss: Tensor, average loss for the batch of data, shape ()\n",
    "    \"\"\"\n",
    "    return -logits[range(targets.shape[0]), targets].float().mean()\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n",
      "tensor(-4.)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.arange(9).reshape(3, 3)\n",
    "targets = torch.tensor([2, 1, 0])\n",
    "targets_one_hot = torch.zeros_like(logits)\n",
    "targets_one_hot[range(targets.shape[0]), targets] = 1\n",
    "print(targets_one_hot)\n",
    "loss = nll_loss(logits, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0781)\n",
      "tensor(0.0673)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "print(get_accuracy(model(x_train[:batch_size]), y_train[:batch_size]))\n",
    "print(get_accuracy(model(x_train), y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.40359678864479065\n",
      "acc_train: 0.9051399827003479\n",
      "Epoch 1:\n",
      "loss_train: 0.3132064938545227\n",
      "acc_train: 0.9135199785232544\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(int(np.ceil((n_train / batch_size)))):\n",
    "        # Get minibatch \n",
    "        start_i = i * batch_size\n",
    "        end_i = min(start_i + batch_size, n_train)\n",
    "        x_mini = x_train[start_i:end_i]\n",
    "        y_mini = y_train[start_i:end_i]\n",
    "\n",
    "        # Generate predictions\n",
    "        logits = model(x_mini)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, y_mini)\n",
    "\n",
    "        # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "        loss.backward()\n",
    "\n",
    "        # Do gradient descent\n",
    "        with torch.no_grad():\n",
    "            weight -= learning_rate * weight.grad\n",
    "            bias -= learning_rate * bias.grad\n",
    "        weight.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    print(\"loss_train: {}\".format(loss))\n",
    "    print(\"acc_train: {}\".format(get_accuracy(model(x_train), y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    return input @ weight + bias\n",
    "\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2299, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(x_train[:batch_size]), y_train[:batch_size])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        # no need to set require_grad = True\n",
    "        self.weight = nn.Parameter(torch.empty(num_features, num_classes).uniform_(-1, 1) \n",
    "                            * np.sqrt(6 / (num_features + num_classes)))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input @ self.weight + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3475, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_features, num_classes)\n",
    "print(loss_fn(model(x_train), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "def fit(model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil((n_train / batch_size)))):\n",
    "            # Get minibatch \n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, n_train)\n",
    "            x_mini = x_train[start_i:end_i]\n",
    "            y_mini = y_train[start_i:end_i]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(x_mini)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y_mini)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    param -= learning_rate * param.grad\n",
    "    \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"loss_train: {}\".format(loss))\n",
    "        print(\"acc_train: {}\".format(get_accuracy(model(x_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.26737791299819946\n",
      "acc_train: 0.9166799783706665\n",
      "Epoch 1:\n",
      "loss_train: 0.239226296544075\n",
      "acc_train: 0.9184799790382385\n"
     ]
    }
   ],
   "source": [
    "fit(model=model, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.lin(input)\n",
    "\n",
    "def initialize_weight(model):\n",
    "    if isinstance(model, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.xavier_normal_(model.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.4025786519050598\n",
      "acc_train: 0.90420001745224\n",
      "Epoch 1:\n",
      "loss_train: 0.31374311447143555\n",
      "acc_train: 0.9128599762916565\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_features, num_classes)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "fit(model, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "def fit(model, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil((n_train / batch_size)))):\n",
    "            # Get minibatch \n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, n_train)\n",
    "            x_mini = x_train[start_i:end_i]\n",
    "            y_mini = y_train[start_i:end_i]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(x_mini)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y_mini)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()\n",
    "    \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"loss_train: {}\".format(loss))\n",
    "        print(\"acc_train: {}\".format(get_accuracy(model(x_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.2701719105243683\n",
      "acc_train: 0.9164800047874451\n",
      "Epoch 1:\n",
      "loss_train: 0.24144691228866577\n",
      "acc_train: 0.9181600213050842\n"
     ]
    }
   ],
   "source": [
    "fit(model, optimizer, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "def fit(model, optimizer, train_set, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(int(np.ceil((n_train / batch_size)))):\n",
    "            # Get minibatch \n",
    "            start_i = i * batch_size\n",
    "            end_i = min(start_i + batch_size, n_train)\n",
    "            # x_mini = x_train[start_i:end_i]\n",
    "            # y_mini = y_train[start_i:end_i]\n",
    "            x_mini, y_mini = train_set[start_i:end_i]\n",
    "\n",
    "            # Generate predictions\n",
    "            logits = model(x_mini)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y_mini)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()\n",
    "    \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"loss_train: {}\".format(loss))\n",
    "        print(\"acc_train: {}\".format(get_accuracy(model(x_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.40804043412208557\n",
      "acc_train: 0.9044600129127502\n",
      "Epoch 1:\n",
      "loss_train: 0.3180721402168274\n",
      "acc_train: 0.9131600260734558\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_features, num_classes)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, train_set, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "def fit(model, optimizer, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for (x_mini, y_mini) in train_loader:\n",
    "            # Generate predictions\n",
    "            logits = model(x_mini)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y_mini)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()\n",
    "    \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"loss_train: {}\".format(loss))\n",
    "        print(\"acc_train: {}\".format(get_accuracy(model(x_train), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 1.202134132385254\n",
      "acc_train: 0.8512799739837646\n",
      "Epoch 1:\n",
      "loss_train: 0.2358727604150772\n",
      "acc_train: 0.9149600267410278\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_features, num_classes)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, train_loader, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(x_train, y_train)\n",
    "val_set = TensorDataset(x_val, y_val)\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "dataloaders[\"val\"] = DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "learning_rate = 0.5\n",
    "def fit(model, optimizer, dataloaders, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for (x_mini, y_mini) in dataloaders[\"train\"]:\n",
    "            # Generate predictions\n",
    "            logits = model(x_mini)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(logits, y_mini)\n",
    "\n",
    "            # Compute the gradients of the loss w.r.t parameters (weight, bias)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for (x_mini, y_mini) in dataloaders[\"val\"]:\n",
    "                logits = model(x_mini)\n",
    "\n",
    "                loss_val += loss_fn(logits, y_mini)\n",
    "            avg_loss = loss_val / len(dataloaders[\"val\"])\n",
    "    \n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"loss_train: {}\".format(loss))\n",
    "        print(\"loss_val: {}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "loss_train: 0.3111090362071991\n",
      "loss_val: 0.36481213569641113\n",
      "Epoch 1:\n",
      "loss_train: 0.14805105328559875\n",
      "loss_val: 0.2823346257209778\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_features, num_classes)\n",
    "model.apply(initialize_weight)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(model, optimizer, dataloaders, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7defa187d5e3710339268a5cd15ef0cca1b3c8a600ebfb440211335c2fae9506"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('i2dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
